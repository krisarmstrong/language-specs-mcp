{"language": "julia", "generatedAt": "2026-01-06T18:23:22.307292+00:00", "entries": [{"path": "generation-checklist.md", "category": "spec", "name": "generation-checklist", "content": "# Julia Generation Checklist\n\n**Read this BEFORE writing Julia code. Type stability and performance matter.**\n\n## Critical: You Must Do These\n\n### 1. Write Type-Stable Functions\n```julia\n# BAD - type unstable (returns different types)\nfunction bad_abs(x)\n    if x > 0\n        return x\n    else\n        return -x  # Could be different type if x is unsigned\n    end\nend\n\n# GOOD - type stable\nfunction good_abs(x::T) where T <: Number\n    return x > zero(T) ? x : -x\nend\n\n# Check type stability with @code_warntype\n@code_warntype good_abs(-5)\n```\n\n### 2. Avoid Global Variables in Performance-Critical Code\n```julia\n# BAD - global variable causes type instability\nx = 10\nfunction bad_compute()\n    return x * 2  # x type unknown at compile time\nend\n\n# GOOD - pass as argument\nfunction good_compute(x)\n    return x * 2\nend\n\n# GOOD - const for truly constant globals\nconst CONFIG_VALUE = 10\nfunction compute_with_const()\n    return CONFIG_VALUE * 2  # OK - type is known\nend\n```\n\n### 3. Pre-allocate Arrays\n```julia\n# BAD - growing array\nfunction bad_squares(n)\n    result = []\n    for i in 1:n\n        push!(result, i^2)\n    end\n    return result\nend\n\n# GOOD - pre-allocated with type\nfunction good_squares(n)\n    result = Vector{Int}(undef, n)\n    for i in 1:n\n        result[i] = i^2\n    end\n    return result\nend\n\n# BETTER - comprehension\nsquares(n) = [i^2 for i in 1:n]\n```\n\n### 4. Use Broadcasting with Dot Syntax\n```julia\n# BAD - explicit loop\nfunction add_arrays_bad(a, b)\n    result = similar(a)\n    for i in eachindex(a)\n        result[i] = a[i] + b[i]\n    end\n    return result\nend\n\n# GOOD - broadcasting\nadd_arrays(a, b) = a .+ b\n\n# GOOD - fused broadcasting (single loop, no temporaries)\nresult = @. sin(x) + cos(y) * z\n# Equivalent to: sin.(x) .+ cos.(y) .* z\n```\n\n### 5. Use Multiple Dispatch Properly\n```julia\n# GOOD - leverage Julia's strength\nabstract type Shape end\n\nstruct Circle <: Shape\n    radius::Float64\nend\n\nstruct Rectangle <: Shape\n    width::Float64\n    height::Float64\nend\n\n# Method for each type\narea(c::Circle) = \u03c0 * c.radius^2\narea(r::Rectangle) = r.width * r.height\n\n# Generic function works with any Shape\ntotal_area(shapes::Vector{<:Shape}) = sum(area, shapes)\n```\n\n## Important: Strong Recommendations\n\n### 6. Use Concrete Types in Struct Fields\n```julia\n# BAD - abstract field type\nstruct BadContainer\n    data::AbstractVector  # Type instability\nend\n\n# GOOD - parametric type\nstruct GoodContainer{T<:AbstractVector}\n    data::T\nend\n\n# Or concrete type if known\nstruct IntContainer\n    data::Vector{Int}\nend\n```\n\n### 7. Use `@views` for Slicing Without Copies\n```julia\n# BAD - creates copy\nfunction process_slice(A)\n    B = A[1:100, :]  # Allocates new array\n    return sum(B)\nend\n\n# GOOD - view (no allocation)\nfunction process_slice_view(A)\n    B = @view A[1:100, :]  # No copy\n    return sum(B)\nend\n\n# GOOD - @views macro for block\nfunction process_views(A)\n    @views begin\n        x = A[1:10]\n        y = A[11:20]\n        return x .+ y\n    end\nend\n```\n\n### 8. Use `eachindex` and `axes` for Iteration\n```julia\n# BAD - assumes 1-based indexing\nfor i in 1:length(A)\n    A[i] = i\nend\n\n# GOOD - works with any array type\nfor i in eachindex(A)\n    A[i] = i\nend\n\n# GOOD - multi-dimensional\nfor i in axes(A, 1), j in axes(A, 2)\n    A[i, j] = i + j\nend\n```\n\n### 9. Use Named Tuples and Keyword Arguments\n```julia\n# BAD - positional args hard to remember\nfunction create_user(name, email, age, active, role)\n    # What order?\nend\n\n# GOOD - keyword arguments\nfunction create_user(; name, email, age=0, active=true, role=\"user\")\n    return (; name, email, age, active, role)  # Named tuple\nend\n\nuser = create_user(name=\"Alice\", email=\"a@b.com\")\n```\n\n### 10. Use `nothing` and Union Types for Optional Values\n```julia\n# GOOD - explicit optional\nfunction find_user(id::Int)::Union{User, Nothing}\n    # Returns User or nothing\nend\n\n# Check with isnothing\nuser = find_user(123)\nif !isnothing(user)\n    println(user.name)\nend\n\n# Or with pattern\nresult = something(find_user(123), default_user)\n```\n\n## Performance\n\n### 11. Use `@inbounds` for Hot Loops (Carefully)\n```julia\n# GOOD - when you've verified bounds are safe\nfunction sum_array(A)\n    s = zero(eltype(A))\n    @inbounds for i in eachindex(A)\n        s += A[i]\n    end\n    return s\nend\n\n# CAUTION: Only use when you're certain indices are valid\n```\n\n### 12. Use `@simd` for Vectorizable Loops\n```julia\n# GOOD - enables SIMD optimization\nfunction dot_product(a, b)\n    s = zero(eltype(a))\n    @simd for i in eachindex(a)\n        @inbounds s += a[i] * b[i]\n    end\n    return s\nend\n```\n\n### 13. Profile Before Optimizing\n```julia\n# Use built-in profiling\nusing Profile\n\n@profile my_function(args)\nProfile.print()\n\n# Use BenchmarkTools for timing\nusing BenchmarkTools\n\n@btime my_function($args)  # $ interpolates to avoid globals\n@benchmark my_function($args)\n```\n\n### 14. Use StaticArrays for Small Fixed-Size Arrays\n```julia\nusing StaticArrays\n\n# GOOD - stack-allocated, very fast for small arrays\nfunction compute_3d(v::SVector{3, Float64})\n    return v[1]^2 + v[2]^2 + v[3]^2\nend\n\nv = SVector(1.0, 2.0, 3.0)\n```\n\n## Best Practices\n\n### 15. Use Modules for Code Organization\n```julia\nmodule MyModule\n\nexport public_function\n\n# Private - not exported\nfunction helper()\n    # ...\nend\n\n# Public\nfunction public_function(x)\n    return helper() + x\nend\n\nend # module\n```\n\n### 16. Handle Errors with Exceptions\n```julia\n# GOOD - custom exception types\nstruct ValidationError <: Exception\n    msg::String\nend\n\nfunction validate(x)\n    x > 0 || throw(ValidationError(\"x must be positive\"))\n    return x\nend\n\n# Handle errors\ntry\n    validate(-1)\ncatch e\n    if e isa ValidationError\n        println(\"Validation failed: \", e.msg)\n    else\n        rethrow()\n    end\nend\n```\n\n---\n\n**Quick Reference - Copy This Mental Model:**\n- Type-stable functions (check with @code_warntype)\n- Avoid globals (or use `const`)\n- Pre-allocate arrays with types\n- Dot broadcasting (`.+`, `@.`)\n- Multiple dispatch for polymorphism\n- Concrete types in struct fields\n- `@views` for slicing\n- `eachindex` for iteration\n- Keyword arguments for clarity\n- `Union{T, Nothing}` for optionals\n- `@inbounds` and `@simd` for hot loops\n- BenchmarkTools for profiling\n- StaticArrays for small fixed arrays\n"}, {"path": "patterns/idioms.md", "category": "patterns", "name": "patterns/idioms", "content": "# Julia Idioms\n\n## Prefer dot broadcasting\n\nUse dot syntax for element-wise operations.\n\n## Type-stable functions\n\nAvoid type instability in hot paths.\n"}, {"path": "formatters/overview.md", "category": "formatters", "name": "formatters/overview", "content": "# Julia Formatters\n\nA common formatter is JuliaFormatter.\n\nSee: https://github.com/domluna/JuliaFormatter.jl\n"}, {"path": "stdlib/overview.md", "category": "stdlib", "name": "stdlib/overview", "content": "# [Linear Algebra](#man-linalg)#man-linalg\n\nIn addition to (and as part of) its support for multi-dimensional arrays, Julia provides native implementations of many common and useful linear algebra operations which can be loaded with `using LinearAlgebra`. Basic operations, such as [tr](#LinearAlgebra.tr), [det](#LinearAlgebra.det), and [inv](../../base/math/#Base.inv-Tuple{Number}) are all supported:\n\n```\njulia> A = [1 2 3; 4 1 6; 7 8 1]\n3\u00d73 Matrix{Int64}:\n 1  2  3\n 4  1  6\n 7  8  1\n\njulia> tr(A)\n3\n\njulia> det(A)\n104.0\n\njulia> inv(A)\n3\u00d73 Matrix{Float64}:\n -0.451923   0.211538    0.0865385\n  0.365385  -0.192308    0.0576923\n  0.240385   0.0576923  -0.0673077\n```\n\nAs well as other useful operations, such as finding eigenvalues or eigenvectors:\n\n```\njulia> A = [-4. -17.; 2. 2.]\n2\u00d72 Matrix{Float64}:\n -4.0  -17.0\n  2.0    2.0\n\njulia> eigvals(A)\n2-element Vector{ComplexF64}:\n -1.0 - 5.0im\n -1.0 + 5.0im\n\njulia> eigvecs(A)\n2\u00d72 Matrix{ComplexF64}:\n  0.945905-0.0im        0.945905+0.0im\n -0.166924+0.278207im  -0.166924-0.278207im\n```\n\nIn addition, Julia provides many [factorizations](#man-linalg-factorizations) which can be used to speed up problems such as linear solve or matrix exponentiation by pre-factorizing a matrix into a form more amenable (for performance or memory reasons) to the problem. See the documentation on [factorize](#LinearAlgebra.factorize) for more information. As an example:\n\n```\njulia> A = [1.5 2 -4; 3 -1 -6; -10 2.3 4]\n3\u00d73 Matrix{Float64}:\n   1.5   2.0  -4.0\n   3.0  -1.0  -6.0\n -10.0   2.3   4.0\n\njulia> factorize(A)\nLU{Float64, Matrix{Float64}, Vector{Int64}}\nL factor:\n3\u00d73 Matrix{Float64}:\n  1.0    0.0       0.0\n -0.15   1.0       0.0\n -0.3   -0.132196  1.0\nU factor:\n3\u00d73 Matrix{Float64}:\n -10.0  2.3     4.0\n   0.0  2.345  -3.4\n   0.0  0.0    -5.24947\n```\n\nSince `A` is not Hermitian, symmetric, triangular, tridiagonal, or bidiagonal, an LU factorization may be the best we can do. Compare with:\n\n```\njulia> B = [1.5 2 -4; 2 -1 -3; -4 -3 5]\n3\u00d73 Matrix{Float64}:\n  1.5   2.0  -4.0\n  2.0  -1.0  -3.0\n -4.0  -3.0   5.0\n\njulia> factorize(B)\nBunchKaufman{Float64, Matrix{Float64}, Vector{Int64}}\nD factor:\n3\u00d73 Tridiagonal{Float64, Vector{Float64}}:\n -1.64286   0.0   \u22c5\n  0.0      -2.8  0.0\n   \u22c5        0.0  5.0\nU factor:\n3\u00d73 UnitUpperTriangular{Float64, Matrix{Float64}}:\n 1.0  0.142857  -0.8\n  \u22c5   1.0       -0.6\n  \u22c5    \u22c5         1.0\npermutation:\n3-element Vector{Int64}:\n 1\n 2\n 3\n```\n\nHere, Julia was able to detect that `B` is in fact symmetric, and used a more appropriate factorization. Often it's possible to write more efficient code for a matrix that is known to have certain properties e.g. it is symmetric, or tridiagonal. Julia provides some special types so that you can \"tag\" matrices as having these properties. For instance:\n\n```\njulia> B = [1.5 2 -4; 2 -1 -3; -4 -3 5]\n3\u00d73 Matrix{Float64}:\n  1.5   2.0  -4.0\n  2.0  -1.0  -3.0\n -4.0  -3.0   5.0\n\njulia> sB = Symmetric(B)\n3\u00d73 Symmetric{Float64, Matrix{Float64}}:\n  1.5   2.0  -4.0\n  2.0  -1.0  -3.0\n -4.0  -3.0   5.0\n```\n\n`sB` has been tagged as a matrix that's (real) symmetric, so for later operations we might perform on it, such as eigenfactorization or computing matrix-vector products, efficiencies can be found by only referencing half of it. For example:\n\n```\njulia> B = [1.5 2 -4; 2 -1 -3; -4 -3 5]\n3\u00d73 Matrix{Float64}:\n  1.5   2.0  -4.0\n  2.0  -1.0  -3.0\n -4.0  -3.0   5.0\n\njulia> sB = Symmetric(B)\n3\u00d73 Symmetric{Float64, Matrix{Float64}}:\n  1.5   2.0  -4.0\n  2.0  -1.0  -3.0\n -4.0  -3.0   5.0\n\njulia> x = [1; 2; 3]\n3-element Vector{Int64}:\n 1\n 2\n 3\n\njulia> sB\\x\n3-element Vector{Float64}:\n -1.7391304347826084\n -1.1086956521739126\n -1.4565217391304346\n```\n\nThe `\\` operation here performs the linear solution. The left-division operator is pretty powerful and it's easy to write compact, readable code that is flexible enough to solve all sorts of systems of linear equations.\n\n## [Special matrices](#Special-matrices)#Special-matrices\n\n[Matrices with special symmetries and structures](https://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274) arise often in linear algebra and are frequently associated with various matrix factorizations. Julia features a rich collection of special matrix types, which allow for fast computation with specialized routines that are specially developed for particular matrix types.\n\nThe following tables summarize the types of special matrices that have been implemented in Julia, as well as whether hooks to various optimized methods for them in LAPACK are available.\n\nTypeDescription[Symmetric](#LinearAlgebra.Symmetric)[Symmetric matrix](https://en.wikipedia.org/wiki/Symmetric_matrix)[Hermitian](#LinearAlgebra.Hermitian)[Hermitian matrix](https://en.wikipedia.org/wiki/Hermitian_matrix)[UpperTriangular](#LinearAlgebra.UpperTriangular)Upper [triangular matrix](https://en.wikipedia.org/wiki/Triangular_matrix)[UnitUpperTriangular](#LinearAlgebra.UnitUpperTriangular)Upper [triangular matrix](https://en.wikipedia.org/wiki/Triangular_matrix) with unit diagonal[LowerTriangular](#LinearAlgebra.LowerTriangular)Lower [triangular matrix](https://en.wikipedia.org/wiki/Triangular_matrix)[UnitLowerTriangular](#LinearAlgebra.UnitLowerTriangular)Lower [triangular matrix](https://en.wikipedia.org/wiki/Triangular_matrix) with unit diagonal[UpperHessenberg](#LinearAlgebra.UpperHessenberg)Upper [Hessenberg matrix](https://en.wikipedia.org/wiki/Hessenberg_matrix)[Tridiagonal](#LinearAlgebra.Tridiagonal)[Tridiagonal matrix](https://en.wikipedia.org/wiki/Tridiagonal_matrix)[SymTridiagonal](#LinearAlgebra.SymTridiagonal)Symmetric tridiagonal matrix[Bidiagonal](#LinearAlgebra.Bidiagonal)Upper/lower [bidiagonal matrix](https://en.wikipedia.org/wiki/Bidiagonal_matrix)[Diagonal](#LinearAlgebra.Diagonal)[Diagonal matrix](https://en.wikipedia.org/wiki/Diagonal_matrix)[UniformScaling](#LinearAlgebra.UniformScaling)[Uniform scaling operator](https://en.wikipedia.org/wiki/Uniform_scaling)\n\n### [Elementary operations](#Elementary-operations)#Elementary-operations\n\nMatrix type`+``-``*``\\`Other functions with optimized methods[Symmetric](#LinearAlgebra.Symmetric)MV[inv](../../base/math/#Base.inv-Tuple{Number}), [sqrt](../../base/math/#Base.sqrt-Tuple{Number}), [cbrt](../../base/math/#Base.Math.cbrt-Tuple{AbstractFloat}), [exp](../../base/math/#Base.exp-Tuple{Float64})[Hermitian](#LinearAlgebra.Hermitian)MV[inv](../../base/math/#Base.inv-Tuple{Number}), [sqrt](../../base/math/#Base.sqrt-Tuple{Number}), [cbrt](../../base/math/#Base.Math.cbrt-Tuple{AbstractFloat}), [exp](../../base/math/#Base.exp-Tuple{Float64})[UpperTriangular](#LinearAlgebra.UpperTriangular)MVMV[inv](../../base/math/#Base.inv-Tuple{Number}), [det](#LinearAlgebra.det), [logdet](#LinearAlgebra.logdet)[UnitUpperTriangular](#LinearAlgebra.UnitUpperTriangular)MVMV[inv](../../base/math/#Base.inv-Tuple{Number}), [det](#LinearAlgebra.det), [logdet](#LinearAlgebra.logdet)[LowerTriangular](#LinearAlgebra.LowerTriangular)MVMV[inv](../../base/math/#Base.inv-Tuple{Number}), [det](#LinearAlgebra.det), [logdet](#LinearAlgebra.logdet)[UnitLowerTriangular](#LinearAlgebra.UnitLowerTriangular)MVMV[inv](../../base/math/#Base.inv-Tuple{Number}), [det](#LinearAlgebra.det), [logdet](#LinearAlgebra.logdet)[UpperHessenberg](#LinearAlgebra.UpperHessenberg)MM[inv](../../base/math/#Base.inv-Tuple{Number}), [det](#LinearAlgebra.det)[SymTridiagonal](#LinearAlgebra.SymTridiagonal)MMMSMV[eigmax](#LinearAlgebra.eigmax), [eigmin](#LinearAlgebra.eigmin)[Tridiagonal](#LinearAlgebra.Tridiagonal)MMMSMV[Bidiagonal](#LinearAlgebra.Bidiagonal)MMMSMV[Diagonal](#LinearAlgebra.Diagonal)MMMVMV[inv](../../base/math/#Base.inv-Tuple{Number}), [det](#LinearAlgebra.det), [logdet](#LinearAlgebra.logdet), [/](../../base/math/#Base.:/)[UniformScaling](#LinearAlgebra.UniformScaling)MMMVSMVS[/](../../base/math/#Base.:/)\n\nLegend:\n\nKeyDescriptionM (matrix)An optimized method for matrix-matrix operations is availableV (vector)An optimized method for matrix-vector operations is availableS (scalar)An optimized method for matrix-scalar operations is available\n\n### [Matrix factorizations](#Matrix-factorizations)#Matrix-factorizations\n\nMatrix typeLAPACK[eigen](#LinearAlgebra.eigen)[eigvals](#LinearAlgebra.eigvals)[eigvecs](#LinearAlgebra.eigvecs)[svd](#LinearAlgebra.svd)[svdvals](#LinearAlgebra.svdvals)[Symmetric](#LinearAlgebra.Symmetric)SYARI[Hermitian](#LinearAlgebra.Hermitian)HEARI[UpperTriangular](#LinearAlgebra.UpperTriangular)TRAAA[UnitUpperTriangular](#LinearAlgebra.UnitUpperTriangular)TRAAA[LowerTriangular](#LinearAlgebra.LowerTriangular)TRAAA[UnitLowerTriangular](#LinearAlgebra.UnitLowerTriangular)TRAAA[SymTridiagonal](#LinearAlgebra.SymTridiagonal)STAARIAV[Tridiagonal](#LinearAlgebra.Tridiagonal)GT[Bidiagonal](#LinearAlgebra.Bidiagonal)BDAA[Diagonal](#LinearAlgebra.Diagonal)DIA\n\nLegend:\n\nKeyDescriptionExampleA (all)An optimized method to find all the characteristic values and/or vectors is availablee.g. `eigvals(M)`R (range)An optimized method to find the `il`th through the `ih`th characteristic values are available`eigvals(M, il, ih)`I (interval)An optimized method to find the characteristic values in the interval [`vl`, `vh`] is available`eigvals(M, vl, vh)`V (vectors)An optimized method to find the characteristic vectors corresponding to the characteristic values `x=[x1, x2,...]` is available`eigvecs(M, x)`\n\n### [The uniform scaling operator](#The-uniform-scaling-operator)#The-uniform-scaling-operator\n\nA [UniformScaling](#LinearAlgebra.UniformScaling) operator represents a scalar times the identity operator, `\u03bb*I`. The identity operator `I` is defined as a constant and is an instance of `UniformScaling`. The size of these operators are generic and match the other matrix in the binary operations [+](../../base/math/#Base.:+), [-](../../base/math/#Base.:--Tuple{Any}), [*](../../base/math/#Base.:*-Tuple{Any, Vararg{Any}}) and [\\](../../base/math/#Base.:\\\\-Tuple{Any, Any}). For `A+I` and `A-I` this means that `A` must be square. Multiplication with the identity operator `I` is a noop (except for checking that the scaling factor is one) and therefore almost without overhead.\n\nTo see the `UniformScaling` operator in action:\n\n```\njulia> U = UniformScaling(2);\n\njulia> a = [1 2; 3 4]\n2\u00d72 Matrix{Int64}:\n 1  2\n 3  4\n\njulia> a + U\n2\u00d72 Matrix{Int64}:\n 3  2\n 3  6\n\njulia> a * U\n2\u00d72 Matrix{Int64}:\n 2  4\n 6  8\n\njulia> [a U]\n2\u00d74 Matrix{Int64}:\n 1  2  2  0\n 3  4  0  2\n\njulia> b = [1 2 3; 4 5 6]\n2\u00d73 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\njulia> b - U\nERROR: DimensionMismatch: matrix is not square: dimensions are (2, 3)\nStacktrace:\n[...]\n```\n\nIf you need to solve many systems of the form `(A+\u03bcI)x = b` for the same `A` and different `\u03bc`, it might be beneficial to first compute the Hessenberg factorization `F` of `A` via the [hessenberg](#LinearAlgebra.hessenberg) function. Given `F`, Julia employs an efficient algorithm for `(F+\u03bc*I) \\ b` (equivalent to `(A+\u03bc*I)x \\ b`) and related operations like determinants.\n\n## [Matrix factorizations](#man-linalg-factorizations)#man-linalg-factorizations\n\n[Matrix factorizations (a.k.a. matrix decompositions)](https://en.wikipedia.org/wiki/Matrix_decomposition) compute the factorization of a matrix into a product of matrices, and are one of the central concepts in (numerical) linear algebra.\n\nThe following table summarizes the types of matrix factorizations that have been implemented in Julia. Details of their associated methods can be found in the [Standard functions](#Standard-functions) section of the Linear Algebra documentation.\n\nTypeDescription`BunchKaufman`Bunch-Kaufman factorization`Cholesky`[Cholesky factorization](https://en.wikipedia.org/wiki/Cholesky_decomposition)`CholeskyPivoted`[Pivoted](https://en.wikipedia.org/wiki/Pivot_element) Cholesky factorization`LDLt`[LDL(T) factorization](https://en.wikipedia.org/wiki/Cholesky_decomposition#LDL_decomposition)`LU`[LU factorization](https://en.wikipedia.org/wiki/LU_decomposition)`QR`[QR factorization](https://en.wikipedia.org/wiki/QR_decomposition)`QRCompactWY`Compact WY form of the QR factorization`QRPivoted`Pivoted [QR factorization](https://en.wikipedia.org/wiki/QR_decomposition)`LQ`[QR factorization](https://en.wikipedia.org/wiki/QR_decomposition) of `transpose(A)``Hessenberg`[Hessenberg decomposition](https://mathworld.wolfram.com/HessenbergDecomposition.html)`Eigen`[Spectral decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix)`GeneralizedEigen`[Generalized spectral decomposition](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Generalized_eigenvalue_problem)`SVD`[Singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)`GeneralizedSVD`[Generalized SVD](https://en.wikipedia.org/wiki/Generalized_singular_value_decomposition#Higher_order_version)`Schur`[Schur decomposition](https://en.wikipedia.org/wiki/Schur_decomposition)`GeneralizedSchur`[Generalized Schur decomposition](https://en.wikipedia.org/wiki/Schur_decomposition#Generalized_Schur_decomposition)\n\nAdjoints and transposes of [Factorization](#LinearAlgebra.Factorization) objects are lazily wrapped in `AdjointFactorization` and `TransposeFactorization` objects, respectively. Generically, transpose of real `Factorization`s are wrapped as `AdjointFactorization`.\n\n## [Orthogonal matrices (AbstractQ)](#man-linalg-abstractq)#man-linalg-abstractq\n\nSome matrix factorizations generate orthogonal/unitary \"matrix\" factors. These factorizations include QR-related factorizations obtained from calls to [qr](#LinearAlgebra.qr), i.e., `QR`, `QRCompactWY` and `QRPivoted`, the Hessenberg factorization obtained from calls to [hessenberg](#LinearAlgebra.hessenberg), and the LQ factorization obtained from [lq](#LinearAlgebra.lq). While these orthogonal/unitary factors admit a matrix representation, their internal representation is, for performance and memory reasons, different. Hence, they should be rather viewed as matrix-backed, function-based linear operators. In particular, reading, for instance, a column of its matrix representation requires running \"matrix\"-vector multiplication code, rather than simply reading out data from memory (possibly filling parts of the vector with structural zeros). Another clear distinction from other, non-triangular matrix types is that the underlying multiplication code allows for in-place modification during multiplication. Furthermore, objects of specific `AbstractQ` subtypes as those created via [qr](#LinearAlgebra.qr), [hessenberg](#LinearAlgebra.hessenberg) and [lq](#LinearAlgebra.lq) can behave like a square or a rectangular matrix depending on context:\n\n```\njulia> using LinearAlgebra\n\njulia> Q = qr(rand(3,2)).Q\n3\u00d73 LinearAlgebra.QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}\n\njulia> Matrix(Q)\n3\u00d72 Matrix{Float64}:\n -0.320597   0.865734\n -0.765834  -0.475694\n -0.557419   0.155628\n\njulia> Q*I\n3\u00d73 Matrix{Float64}:\n -0.320597   0.865734  -0.384346\n -0.765834  -0.475694  -0.432683\n -0.557419   0.155628   0.815514\n\njulia> Q*ones(2)\n3-element Vector{Float64}:\n  0.5451367118802273\n -1.241527373086654\n -0.40179067589600226\n\njulia> Q*ones(3)\n3-element Vector{Float64}:\n  0.16079054743832022\n -1.674209978965636\n  0.41372375588835797\n\njulia> ones(1,2) * Q'\n1\u00d73 Matrix{Float64}:\n 0.545137  -1.24153  -0.401791\n\njulia> ones(1,3) * Q'\n1\u00d73 Matrix{Float64}:\n 0.160791  -1.67421  0.413724\n```\n\nDue to this distinction from dense or structured matrices, the abstract `AbstractQ` type does not subtype `AbstractMatrix`, but instead has its own type hierarchy. Custom types that subtype `AbstractQ` can rely on generic fallbacks if the following interface is satisfied. For example, for\n\n```\nstruct MyQ{T} <: LinearAlgebra.AbstractQ{T}\n    # required fields\nend\n```\n\nprovide overloads for\n\n```\nBase.size(Q::MyQ) # size of corresponding square matrix representation\nBase.convert(::Type{AbstractQ{T}}, Q::MyQ) # eltype promotion [optional]\nLinearAlgebra.lmul!(Q::MyQ, x::AbstractVecOrMat) # left-multiplication\nLinearAlgebra.rmul!(A::AbstractMatrix, Q::MyQ) # right-multiplication\n```\n\nIf `eltype` promotion is not of interest, the `convert` method is unnecessary, since by default `convert(::Type{AbstractQ{T}}, Q::AbstractQ{T})` returns `Q` itself. Adjoints of `AbstractQ`-typed objects are lazily wrapped in an `AdjointQ` wrapper type, which requires its own `LinearAlgebra.lmul!` and `LinearAlgebra.rmul!` methods. Given this set of methods, any `Q::MyQ` can be used like a matrix, preferably in a multiplicative context: multiplication via `*` with scalars, vectors and matrices from left and right, obtaining a matrix representation of `Q` via `Matrix(Q)` (or `Q*I`) and indexing into the matrix representation all work. In contrast, addition and subtraction as well as more generally broadcasting over elements in the matrix representation fail because that would be highly inefficient. For such use cases, consider computing the matrix representation up front and cache it for future reuse.\n\n## [Pivoting Strategies](#man-linalg-pivoting-strategies)#man-linalg-pivoting-strategies\n\nSeveral of Julia's [matrix factorizations](#man-linalg-factorizations) support [pivoting](https://en.wikipedia.org/wiki/Pivot_element), which can be used to improve their numerical stability. In fact, some matrix factorizations, such as the LU factorization, may fail without pivoting.\n\nIn pivoting, first, a [pivot element](https://en.wikipedia.org/wiki/Pivot_element) with good numerical properties is chosen based on a pivoting strategy. Next, the rows and columns of the original matrix are permuted to bring the chosen element in place for subsequent computation. Furthermore, the process is repeated for each stage of the factorization.\n\nConsequently, besides the conventional matrix factors, the outputs of pivoted factorization schemes also include permutation matrices.\n\nIn the following, the pivoting strategies implemented in Julia are briefly described. Note that not all matrix factorizations may support them. Consult the documentation of the respective [matrix factorization](#man-linalg-factorizations) for details on the supported pivoting strategies.\n\nSee also [LinearAlgebra.ZeroPivotException](#LinearAlgebra.ZeroPivotException).\n\n[LinearAlgebra.NoPivot](#LinearAlgebra.NoPivot) \u2014 Type\n\n```\nNoPivot\n```\n\nPivoting is not performed. This is the default strategy for [cholesky](#LinearAlgebra.cholesky) and [qr](#LinearAlgebra.qr) factorizations. Note, however, that other matrix factorizations such as the LU factorization may fail without pivoting, and may also be numerically unstable for floating-point matrices in the face of roundoff error. In such cases, this pivot strategy is mainly useful for pedagogical purposes.\n"}, {"path": "spec.md", "category": "spec", "name": "spec", "content": "# Julia 1.12 Documentation\nVersion: unknown\n\nSource: https://docs.julialang.org/en/v1/\n\n\nWelcome to the documentation for Julia 1.12.\n\nPlease read the [release notes](NEWS/) to see what has changed since the last release.\n\nNote#Note-2773e04867de8fae\n\nThe documentation is also available in PDF format: [julia-1.12.3.pdf](https://raw.githubusercontent.com/JuliaLang/docs.julialang.org/assets/julia-1.12.3.pdf).\n\n## [Introduction](#man-introduction)#man-introduction\n\nScientific computing has traditionally required the highest performance, yet domain experts have largely moved to slower dynamic languages for daily work. We believe there are many good reasons to prefer dynamic languages for these applications, and we do not expect their use to diminish. Fortunately, modern language design and compiler techniques make it possible to mostly eliminate the performance trade-off and provide a single environment productive enough for prototyping and efficient enough for deploying performance-intensive applications. The Julia programming language fills this role: it is a flexible dynamic language, appropriate for scientific and numerical computing, with performance comparable to traditional statically-typed languages.\n\nBecause Julia's compiler is different from the interpreters used for languages like Python or R, you may find that Julia's performance is unintuitive at first. If you find that something is slow, we highly recommend reading through the [Performance Tips](manual/performance-tips/#man-performance-tips) section before trying anything else. Once you understand how Julia works, it is easy to write code that is nearly as fast as C.\n\n## [Julia Compared to Other Languages](#man-julia-compared-other-languages)#man-julia-compared-other-languages\n\nJulia features optional typing, multiple dispatch, and good performance, achieved using type inference and [just-in-time (JIT) compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation) (and [optional ahead-of-time compilation](https://github.com/JuliaLang/PackageCompiler.jl)), implemented using [LLVM](https://en.wikipedia.org/wiki/Low_Level_Virtual_Machine). It is multi-paradigm, combining features of imperative, functional, and object-oriented programming. Julia provides ease and expressiveness for high-level numerical computing, in the same way as languages such as R, MATLAB, and Python, but also supports general programming. To achieve this, Julia builds upon the lineage of mathematical programming languages, but also borrows much from popular dynamic languages, including [Lisp](https://en.wikipedia.org/wiki/Lisp_(programming_language)), [Perl](https://en.wikipedia.org/wiki/Perl_(programming_language)), [Python](https://en.wikipedia.org/wiki/Python_(programming_language)), [Lua](https://en.wikipedia.org/wiki/Lua_(programming_language)), and [Ruby](https://en.wikipedia.org/wiki/Ruby_(programming_language)).\n\nThe most significant departures of Julia from typical dynamic languages are:\n\n- The core language imposes very little; [Julia Base and the standard library](base/math/#Base.:--Tuple{Any, Any}) are written in Julia itself, including primitive operations like integer arithmetic\n- A rich language of types for constructing and describing objects, that can also optionally be used to make type declarations\n- The ability to define function behavior across many combinations of argument types via [multiple dispatch](https://en.wikipedia.org/wiki/Multiple_dispatch)\n- Automatic generation of efficient, specialized code for different argument types\n- Good performance, approaching that of statically-compiled languages like C\n\nAlthough one sometimes speaks of dynamic languages as being \"typeless\", they are definitely not. Every object, whether primitive or user-defined, has a type. The lack of type declarations in most dynamic languages, however, means that one cannot instruct the compiler about the types of values, and often cannot explicitly talk about types at all. In static languages, on the other hand, while one can \u2013 and usually must \u2013 annotate types for the compiler, types exist only at compile time and cannot be manipulated or expressed at run time. In Julia, types are themselves run-time objects, and can also be used to convey information to the compiler.\n\n### [What Makes Julia, Julia?](#man-what-makes-julia)#man-what-makes-julia\n\nWhile the casual programmer need not explicitly use types or multiple dispatch, they are the core unifying features of Julia: functions are defined on different combinations of argument types, and applied by dispatching to the most specific matching definition. This model is a good fit for mathematical programming, where it is unnatural for the first argument to \"own\" an operation as in traditional object-oriented dispatch. Operators are just functions with special notation \u2013 to extend addition to new user-defined data types, you define new methods for the `+` function. Existing code then seamlessly applies to the new data types.\n\nPartly because of run-time type inference (augmented by optional type annotations), and partly because of a strong focus on performance from the inception of the project, Julia's computational efficiency exceeds that of other dynamic languages, and even rivals that of statically-compiled languages. For large scale numerical problems, speed always has been, continues to be, and probably always will be crucial: the amount of data being processed has easily kept pace with Moore's Law over the past decades.\n\n### [Advantages of Julia](#man-advantages-of-julia)#man-advantages-of-julia\n\nJulia aims to create an unprecedented combination of ease-of-use, power, and efficiency in a single language. In addition to the above, some advantages of Julia over comparable systems include:\n\n- Free and open source ([MIT licensed](https://github.com/JuliaLang/julia/blob/master/LICENSE.md))\n- User-defined types are as fast and compact as built-ins\n- No need to vectorize code for performance; devectorized code is fast\n- Designed for parallelism and distributed computation\n- Lightweight \"green\" threading ([coroutines](https://en.wikipedia.org/wiki/Coroutine))\n- Unobtrusive yet powerful type system\n- Elegant and extensible conversions and promotions for numeric and other types\n- Efficient support for [Unicode](https://en.wikipedia.org/wiki/Unicode), including but not limited to [UTF-8](https://en.wikipedia.org/wiki/UTF-8)\n- Call C functions directly (no wrappers or special APIs needed)\n- Powerful shell-like capabilities for managing other processes\n- Lisp-like macros and other metaprogramming facilities\n\n## [Important Links](#man-important-links)#man-important-links\n\nA non-exhaustive list of links that will be useful as you learn and use the Julia programming language:\n\n- [Julia Homepage](https://julialang.org)\n- [Install Julia](https://julialang.org/install/)\n- [Discussion forum](https://discourse.julialang.org)\n- [Julia YouTube](https://www.youtube.com/user/JuliaLanguage)\n- [Find Julia Packages](https://julialang.org/packages/)\n- [Learning Resources](https://julialang.org/learning/)\n"}, {"path": "linters/jet/overview.md", "category": "linters", "name": "linters/jet/overview", "content": "# JET.jl\n\nJET.jl is a code analyzer for Julia that detects bugs and type instabilities.\n\nVersion: 0.9\nSource: https://github.com/aviatesk/JET.jl\n\n## Installation\n\n```julia\nusing Pkg\nPkg.add(\"JET\")\n```\n\n## Usage\n\n```julia\nusing JET\n\n# Analyze a function call\n@report_call sum([1, 2, 3])\n\n# Analyze with test inputs\nreport_call(sum, (Vector{Int},))\n\n# Analyze a file\nreport_file(\"script.jl\")\n\n# Analyze a package\nreport_package(\"MyPackage\")\n```\n\n## Analysis Types\n\n### Error Analysis\nDetects potential runtime errors:\n- Method errors (no matching method)\n- Undefined variable access\n- Type assertion failures\n- Bounds errors\n\n### Optimization Analysis\nDetects type instabilities:\n\n```julia\n@report_opt sum([1, 2, 3])\n```\n\n## Configuration\n\n```julia\n# Custom analysis\nreport_call(\n    my_function,\n    (Int, String);\n    mode = :sound,           # :sound or :typo\n    target_modules = (Main,)\n)\n```\n\n### Analysis Modes\n\n#### :sound\nStricter analysis, may report more issues.\n\n#### :typo (default)\nBalanced analysis, focuses on likely errors.\n\n## Error Types Detected\n\n### MethodError\nNo method matching the given types.\n\n```julia\n# Will detect\nf(x::Int) = x\nf(\"string\")  # No method for String\n```\n\n### UndefVarError\nAccess to undefined variables.\n\n### TypeError\nType assertion failures.\n\n### BoundsError\nArray index out of bounds (limited detection).\n\n### DivideError\nDivision by zero (constant cases).\n\n## Optimization Issues\n\n### Type Instability\nFunction return type varies based on runtime values.\n\n```julia\n# Type unstable\nfunction unstable(x)\n    if x > 0\n        return 1\n    else\n        return 1.0  # Returns Int or Float64\n    end\nend\n```\n\n### Runtime Dispatch\nDynamic dispatch due to unclear types.\n\n## Integration\n\n### VS Code\nJulia extension shows JET diagnostics.\n\n### CI/CD\n```julia\nusing JET\n\n# Fail CI on errors\nresult = report_package(\"MyPackage\")\n@assert isempty(get_reports(result))\n```\n\n## Suppressing Warnings\n\n```julia\n# JET.jl respects `@assume_effects` and other compiler hints\nBase.@assume_effects :nothrow function safe_function()\n    # ...\nend\n```\n\n## StaticLint.jl\n\nAnother Julia linter focused on style:\n\n```julia\nusing StaticLint\nStaticLint.lint_file(\"script.jl\")\n```\n\n### StaticLint Checks\n- Unused variables\n- Missing docstrings\n- Style issues\n- Import analysis\n"}]}